[config]
    "alg_name"          = "dqn"
    "env_name"          = "LunarLanderContinuous-v2"
    "render"            = true
    "solved_reward"     = 300  # stop training if avg_reward > solved_reward
    "log_interval"      = 1  # print avg reward in the interval
    "max_episodes"      =  600 # max training episodes
    "max_timesteps"     = 1500  # max timesteps in one episode
    "update_timestep"   = 4000  # update policy every n timesteps

    "action_std" = 0.5  # constant std for action distribution (Multivariate Normal)
    "K_epochs" = 80  # update policy for K epochs
    "eps_clip" = 0.2  # clip parameter for PPO
    
    "gamma" = 0.99  # discount factor
    "lr" = 0.0003  # parameters for Adam optimizer
    "betas" = [0.9, 0.999]

    "buffer_size" = 512
    "batch_size" = 32
    "update_steps" = 8
    "epsilon" = 0.9
    "device" = "cpu"
